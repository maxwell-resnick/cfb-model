name: data_uploader_daily

on:
  workflow_dispatch:
    inputs:
      force_run:
        description: 'Bypass NY time gate (true/false)'
        required: false
        default: 'false'
      simulate_failure:
        description: 'Force a failure to test notifications (true/false)'
        required: false
        default: 'false'
  schedule:
    - cron: "0 * * * *"  # hourly (UTC)

jobs:
  run:
    runs-on: ubuntu-latest
    env:
      BEARER_TOKEN: ${{ secrets.BEARER_TOKEN }}
      DATABASE_URL: ${{ secrets.DATABASE_URL }}
      DISCORD_WEBHOOK: ${{ secrets.DISCORD_WEBHOOK }}
      FORCE_RUN: ${{ github.event.inputs.force_run }}
      SIM_FAIL:  ${{ github.event.inputs.simulate_failure }}

    steps:
      - name: Time gate (run only 07–22 America/New_York, unless forced)
        id: timegate
        run: |
          if [ "${FORCE_RUN}" = "true" ]; then
            echo "run=true" >> $GITHUB_OUTPUT
            echo "Force-run enabled; skipping time window check."
            exit 0
          fi
          HOUR_NY=$(TZ=America/New_York date +%H)
          echo "Local NY hour: $HOUR_NY"
          if [ "$HOUR_NY" -ge 7 ] && [ "$HOUR_NY" -le 22 ]; then
            echo "run=true" >> $GITHUB_OUTPUT
          else
            echo "run=false" >> $GITHUB_OUTPUT
            echo "Outside 07–22 NY window. Skipping."
          fi

      - uses: actions/checkout@v4
        if: steps.timegate.outputs.run == 'true'

      - uses: actions/setup-python@v5
        if: steps.timegate.outputs.run == 'true'
        with:
          python-version: "3.11"

      - name: Install deps
        if: steps.timegate.outputs.run == 'true'
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
            pip install requests  # ensure available
          else
            pip install cfbd pandas numpy SQLAlchemy pg8000 python-dotenv certifi filelock requests
          fi

      - name: Snapshot DB (before)
        if: steps.timegate.outputs.run == 'true'
        env:
          DATABASE_URL: ${{ env.DATABASE_URL }}
          PGUSER:       ${{ secrets.PGUSER }}
          PGPASSWORD:   ${{ secrets.PGPASSWORD }}
          PGHOST:       ${{ secrets.PGHOST }}
          PGDATABASE:   ${{ secrets.PGDATABASE }}
        run: |
          date -u +%Y-%m-%dT%H:%M:%SZ > start_ts.txt
          python - <<'PY'
          import os, pandas as pd, urllib.parse
          from sqlalchemy import create_engine, text
          from sqlalchemy.engine import URL

          def load_url():
              raw = (os.environ.get("DATABASE_URL") or "").strip().strip('"').strip("'")
              if raw and "://" in raw:
                  return raw
              # Fallback to component envs if full URL not present
              user = os.environ.get("PGUSER")
              pwd  = os.environ.get("PGPASSWORD")
              host = os.environ.get("PGHOST")
              db   = os.environ.get("PGDATABASE")
              if all([user, pwd, host, db]):
                  return URL.create(
                      drivername="postgresql+pg8000",
                      username=user,
                      password=pwd,
                      host=host,
                      database=db,
                  )
              raise SystemExit("DATABASE_URL missing/invalid and component vars not provided.")

          url = load_url()
          eng = create_engine(url)
          with eng.connect() as conn:
              df = pd.read_sql(text('SELECT * FROM "PreparedData"'), conn)
          df.to_csv('prepared_before.csv', index=False)
          PY

      - name: Run uploader
        if: steps.timegate.outputs.run == 'true'
        id: run_uploader
        run: |
          set -o pipefail
          python data_uploader.py 2>&1 | tee uploader.log

      - name: Parse upsert count
        if: steps.timegate.outputs.run == 'true'
        id: stats
        run: |
          UPS=$(grep -Eo 'Upserted rows: [0-9]+' uploader.log | awk '{print $3}' | tail -1)
          UPS=${UPS:-0}
          echo "upserted=$UPS" >> $GITHUB_OUTPUT
          echo "Parsed Upserted rows: $UPS"

      # ===== Pull only rows touched AFTER the run (date_updated) =====
      - name: Snapshot DB (after, changed rows only)
        if: steps.timegate.outputs.run == 'true'
        env:
          DATABASE_URL: ${{ env.DATABASE_URL }}
          PGUSER:       ${{ secrets.PGUSER }}
          PGPASSWORD:   ${{ secrets.PGPASSWORD }}
          PGHOST:       ${{ secrets.PGHOST }}
          PGDATABASE:   ${{ secrets.PGDATABASE }}
        run: |
          START_TS=$(cat start_ts.txt)
          echo "Start TS: $START_TS"
          python - <<'PY'
          import os, pandas as pd
          from sqlalchemy import create_engine, text
          from sqlalchemy.engine import URL

          def load_url():
              raw = (os.environ.get("DATABASE_URL") or "").strip().strip('"').strip("'")
              if raw and "://" in raw:
                  return raw
              user = os.environ.get("PGUSER")
              pwd  = os.environ.get("PGPASSWORD")
              host = os.environ.get("PGHOST")
              db   = os.environ.get("PGDATABASE")
              if all([user, pwd, host, db]):
                  return URL.create(
                      drivername="postgresql+pg8000",
                      username=user,
                      password=pwd,
                      host=host,
                      database=db,
                  )
              raise SystemExit("DATABASE_URL missing/invalid and component vars not provided.")

          url = load_url()
          start_ts = open('start_ts.txt').read().strip()
          eng = create_engine(url)
          with eng.connect() as conn:
              df = pd.read_sql(
                  text('SELECT * FROM "PreparedData" WHERE date_updated >= :ts'),
                  conn, params={'ts': start_ts}
              )
          df.to_csv('prepared_after_changed.csv', index=False)
          PY

      # ===== Compute per-cell diffs on (id, team, opponent) =====
      - name: Compute per-cell diffs
        if: steps.timegate.outputs.run == 'true'
        run: |
          python - <<'PY'
          import pandas as pd, numpy as np, os, sys
          keys = ['id','team','opponent']

          def safe_read(path):
            if not os.path.exists(path) or os.path.getsize(path) == 0:
              return pd.DataFrame()
            return pd.read_csv(path, low_memory=False)
          before = safe_read('prepared_before.csv')
          after  = safe_read('prepared_after_changed.csv')

          # Prepare outputs
          open('diff_lines.txt','w').close()
          open('new_lines.txt','w').close()
          cols = keys + ['column','old_value','new_value']
          pd.DataFrame(columns=cols).to_csv('changed_cells.csv', index=False)

          if after.empty or before.empty:
              sys.exit(0)

          # Align to common columns, index by composite key
          common_cols = [c for c in after.columns if c in before.columns]
          if not set(keys).issubset(common_cols):
              sys.exit(0)
          before = before[common_cols].set_index(keys)
          after  = after[common_cols].set_index(keys)

          # New rows (in 'after' but not 'before')
          new_idx = after.index.difference(before.index)
          with open('new_lines.txt','w') as f:
              for i,t,o in new_idx:
                  f.write(f"id={i} team={t} opponent={o} NEW\n")

          # Existing rows that changed
          existed = after.index.intersection(before.index)
          rows=[]
          for idx in existed:
              b = before.loc[idx]
              a = after.loc[idx]
              for col in common_cols:
                  if col in keys: continue
                  bv, av = b[col], a[col]
                  same = (pd.isna(bv) and pd.isna(av)) or (bv == av)
                  if not same:
                      rows.append({'id':idx[0],'team':idx[1],'opponent':idx[2],
                                   'column':col,'old_value':bv,'new_value':av})
          diffs = pd.DataFrame(rows)
          diffs.to_csv('changed_cells.csv', index=False)

          def fmt(v):
              if pd.isna(v): return "NA"
              s = str(v).replace("\n"," ")
              return (s[:80] + "…") if len(s) > 81 else s

          if not diffs.empty:
              with open('diff_lines.txt','w') as f:
                  for (i,t,o), g in diffs.groupby(['id','team','opponent']):
                      parts = [f"{c}: {fmt(ov)} -> {fmt(nv)}" for c,ov,nv in zip(g['column'], g['old_value'], g['new_value'])]
                      f.write(f"id={i} team={t} opponent={o} | " + "; ".join(parts) + "\n")
          PY

      - uses: actions/upload-artifact@v4
        if: steps.timegate.outputs.run == 'true'
        with:
          name: changed-cells
          path: |
            changed_cells.csv
            diff_lines.txt
            new_lines.txt
          retention-days: 7

      # --- Test failure on demand
      - name: Simulate failure (for testing notifications)
        if: steps.timegate.outputs.run == 'true' && env.SIM_FAIL == 'true'
        run: |
          echo "Simulating failure as requested." >&2
          exit 1

      # ===== Discord: success (now includes WHAT changed) =====
      - name: Notify Discord (success)
        if: steps.timegate.outputs.run == 'true' && success() && env.DISCORD_WEBHOOK != ''
        env:
          WEBHOOK: ${{ env.DISCORD_WEBHOOK }}
        run: |
          RUN_URL="${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
          NOW=$(date -u +%Y-%m-%dT%H:%M:%SZ)
          UPS="${{ steps.stats.outputs.upserted }}"

          CHANGES=$( (test -f diff_lines.txt && cat diff_lines.txt) | tail -n 12 | sed 's/"/\\"/g' | awk 'BEGIN{ORS="\\n"}{print}')
          NEWROWS=$( (test -f new_lines.txt  && cat new_lines.txt ) | tail -n 12 | sed 's/"/\\"/g' | awk 'BEGIN{ORS="\\n"}{print}')
          [ -z "$CHANGES" ] && CHANGES="(none)"
          [ -z "$NEWROWS" ] && NEWROWS="(none)"

          PAYLOAD=$(cat <<'JSON'
          {
            "username":"CFB Uploader",
            "embeds":[
              {
                "title":"✅ data_uploader succeeded",
                "description":"Branch: **${{ github.ref_name }}**\nUpserted rows: **UPS_PLACEHOLDER**\n\n**Changes (tail)**\n```CHANGES_PLACEHOLDER```\n**New rows (tail)**\n```NEW_PLACEHOLDER```",
                "url":"RUN_URL_PLACEHOLDER",
                "timestamp":"NOW_PLACEHOLDER",
                "color":3066993
              }
            ]
          }
          JSON
          )
          PAYLOAD=${PAYLOAD//UPS_PLACEHOLDER/"$UPS"}
          PAYLOAD=${PAYLOAD//RUN_URL_PLACEHOLDER/"$RUN_URL"}
          PAYLOAD=${PAYLOAD//NOW_PLACEHOLDER/"$NOW"}
          PAYLOAD=${PAYLOAD//CHANGES_PLACEHOLDER/"$CHANGES"}
          PAYLOAD=${PAYLOAD//NEW_PLACEHOLDER/"$NEWROWS"}

          curl -sS -H "Content-Type: application/json" -d "$PAYLOAD" "$WEBHOOK"

      # --- Discord: failure (unchanged)
      - name: Notify Discord (failure)
        if: steps.timegate.outputs.run == 'true' && failure() && env.DISCORD_WEBHOOK != ''
        env:
          WEBHOOK: ${{ env.DISCORD_WEBHOOK }}
        run: |
          RUN_URL="${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
          NOW=$(date -u +%Y-%m-%dT%H:%M:%SZ)
          PAYLOAD=$(cat <<'JSON'
          {"username":"CFB Uploader","embeds":[{"title":"❌ data_uploader failed","description":"Branch: **${{ github.ref_name }}**\nSee run logs for details.","url":"RUN_URL_PLACEHOLDER","timestamp":"NOW_PLACEHOLDER","color":15158332}]}
          JSON
          )
          PAYLOAD=${PAYLOAD//RUN_URL_PLACEHOLDER/"$RUN_URL"}
          PAYLOAD=${PAYLOAD//NOW_PLACEHOLDER/"$NOW"}
          curl -sS -H "Content-Type: application/json" -d "$PAYLOAD" "$WEBHOOK"
